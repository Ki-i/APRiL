{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ki-i/APRiL/blob/master/cnn_classification041101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57QiGF43Vh69",
        "outputId": "39c35ce3-1923-4f5d-ff6e-0cf4c0c52cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52AY7dz9WsC0"
      },
      "outputs": [],
      "source": [
        "workspace_dir = '.'\n",
        "#!unzip -q \"/content/drive/My Drive/crypko_data.zip\" -d \"{workspace_dir}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZHLjPEPEW0iE"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import MSELoss\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import scipy.io as scio\n",
        "import pylab\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G7ydcVOPsj77"
      },
      "outputs": [],
      "source": [
        "class DnCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DnCNN, self).__init__()\n",
        "        channels=1\n",
        "        num_of_layers=10\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 64\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(in_channels=channels, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        for _ in range(num_of_layers-2):\n",
        "            layers.append(nn.Conv2d(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, bias=False))\n",
        "            layers.append(nn.BatchNorm2d(features))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "        layers.append(nn.Conv2d(in_channels=features, out_channels=channels, kernel_size=kernel_size, padding=padding, bias=False))\n",
        " \n",
        "        self.dncnn = nn.Sequential(*layers)\n",
        "        self.fc1=nn.Linear( 1*50*100,6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(6,2)\n",
        "        self.dropout = nn.Dropout(p=0.3)  # dropout训练\n",
        "    def forward(self, x):\n",
        "        y = self.dncnn(x)\n",
        "        #print(y.size())\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropout(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        #print(y.size())\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-AX1zF1JW_xw"
      },
      "outputs": [],
      "source": [
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(64*2*9, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "        self.dropout = nn.Dropout(p=0.3)  # dropout训练\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.relu1(y)\n",
        "        y = self.pool1(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.relu2(y)\n",
        "        y = self.pool2(y)\n",
        "        y = self.conv3(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.pool3(y)\n",
        "        #print(y.size())\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = self.fc1(y)\n",
        "        y = self.dropout(y)\n",
        "        y = self.relu3(y)\n",
        "        y = self.fc2(y)\n",
        "        # y = self.relu4(y)\n",
        "        # y = self.fc3(y)\n",
        "        # y = self.relu5(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLNyO4Z-XAwy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vFcTvIaXGG2"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset_path, fm, dev_ratio):\n",
        "    print(\"load data from path1:\", dataset_path)\n",
        "    data = scio.loadmat(os.path.join(dataset_path, fm))\n",
        "\n",
        "    del data['__header__']\n",
        "    del data['__globals__']\n",
        "    del data['__version__']\n",
        "    # print(x_data.keys())\n",
        "    # print(y_data.keys())\n",
        "    # print(int(len(x_data)/3))\n",
        "    #datalen = int(len(x_data) / 3)\n",
        "    datalen=1000\n",
        "    x = np.zeros((datalen, 1, 50, 100), dtype=np.float)\n",
        "    y = np.zeros(datalen, dtype=np.uint8)\n",
        "    for i in range(1, datalen):\n",
        "        xkey = 'x' + str(i)\n",
        "        #print(xkey)\n",
        "        xtemp = data[xkey]\n",
        "        x[i] = xtemp[1]\n",
        "        \n",
        "        ykey = 'y' + str(i)\n",
        "        y[i] = data[ykey]\n",
        "        #if(i<=datalen/2):\n",
        "        #  y[i] = 1#噪声\n",
        "        #else:\n",
        "        #  y[i] = 0\n",
        "        #if i==1:\n",
        "        #  print(x[1])\n",
        "        #  print(y_data[ykey])\n",
        "        #  print(y[1])\n",
        "        \n",
        "\n",
        "    data_size = len(y)\n",
        "    train_size = int(data_size * (1 - dev_ratio))\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(x)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(y)\n",
        "    # print(\"train size:\", train_size)\n",
        "    # print(\"dev size:\", data_size - train_size)\n",
        "    x_train = x[:train_size]\n",
        "    y_train = y[:train_size]\n",
        "    x_dev = x[train_size:]\n",
        "    y_dev = y[train_size:]\n",
        "    return x_train, y_train, x_dev, y_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DYoliGW6XJJv"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x, self.y = x, y\n",
        "        self.data_size = len(self.y)\n",
        "        #norm_mean = [0.485, 0.456, 0.406]\n",
        "        #norm_std = [0.229, 0.224, 0.225]\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(norm_mean, norm_std),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        '''\n",
        "        这个函数是关键，通过item(索引)来取数据集中的数据，\n",
        "        一般来说在这里才将图像数据加载入内存，之前存的是图像的保存路径\n",
        "        '''\n",
        "        ycut=self.y[item]\n",
        "        #ycut=ycut[101:-6:400,1:-5:500]\n",
        "\n",
        "        label = torch.tensor(ycut,dtype=torch.long)\n",
        "       \n",
        "        #label = torch.reshape(label, (1, -1))\n",
        "        xcut = self.x[item]\n",
        "        #xcut = xcut[101:-6:400,1:-5:500]\n",
        "        x = torch.from_numpy(xcut)\n",
        "        #x=x.unsqueeze(0)\n",
        "        #label=label.squeeze(0)\n",
        "        x=x.float()\n",
        "       \n",
        "        x = torch.div(x, 255.)\n",
        "      \n",
        "        #print(x.size())\n",
        "        #print(label)\n",
        "        #label=torch.div(label, 255.)\n",
        "        return x, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxGk2NrCqPnk",
        "outputId": "aa438a2f-c75a-4c2d-d9a5-aadefa6a4c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.25 0.5  0.75 1.  ]\n"
          ]
        }
      ],
      "source": [
        "x=np.array([1,2,3,4])\n",
        "x=x/4\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1coYPwD6v79d"
      },
      "outputs": [],
      "source": [
        "def psnr(target_data, ref_data):\n",
        "    # target:目标图像  ref:参考图像  scale:尺寸大小\n",
        "    # assume RGB image\n",
        "    #target_data = np.array(target)\n",
        "    #target_data = target_data[scale:-scale, scale:-scale]\n",
        "\n",
        "    #ref_data = np.array(ref)\n",
        "    #ref_data = ref_data[scale:-scale, scale:-scale]\n",
        "    im = ref_data.max()\n",
        "    print('参考图像峰值', ref_data.max(), ref_data.min())\n",
        "    print('实际图像峰值', target_data.max(), target_data.min())\n",
        "    target_data = target_data * (ref_data.max() / target_data.max())\n",
        "    #print('实际图像峰值', target_data.max(), target_data.min())\n",
        "    diff = ref_data - target_data\n",
        "    diff = diff.flatten('C')\n",
        "\n",
        "    #rmse = math.sqrt(np.mean(diff ** 2.))\n",
        "    #return 20 * math.log10(math.pow(im,2) / rmse)\n",
        "    mse = np.mean(diff ** 2.)\n",
        "    return 20 * math.log10(math.pow(im,2) / mse)\n",
        "\n",
        "def ab_err(target_data, ref_data):\n",
        "  diff = abs(ref_data - target_data)/ref_data\n",
        "  diff=diff.cpu().data.numpy()\n",
        "  tdiff=diff[0:,0:2]\n",
        "  vdiff=diff[0:,3:5]\n",
        "  \n",
        "  \n",
        "  terr = np.mean(tdiff)\n",
        "  verr = np.mean(vdiff)\n",
        "\n",
        "  return terr,verr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rl5D9ZN0XThY",
        "outputId": "9ad6c1fa-1a47-4de0-eb45-19195ef32509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load data from path1: /content/drive/My Drive/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0.0000 train acc: 0.4975,train loss: 0.7285, dev acc: 0.4988, dev loss: 0.7149\n",
            "epoch 1.0000 train acc: 0.5663,train loss: 0.6878, dev acc: 0.4988, dev loss: 0.6946\n",
            "epoch 2.0000 train acc: 0.5675,train loss: 0.6811, dev acc: 0.4988, dev loss: 0.6997\n",
            "epoch 3.0000 train acc: 0.5850,train loss: 0.6873, dev acc: 0.5025, dev loss: 0.6955\n",
            "epoch 4.0000 train acc: 0.6362,train loss: 0.6638, dev acc: 0.6575, dev loss: 0.6524\n",
            "epoch 5.0000 train acc: 0.6813,train loss: 0.6428, dev acc: 0.7612, dev loss: 0.6287\n",
            "epoch 6.0000 train acc: 0.6937,train loss: 0.6384, dev acc: 0.8263, dev loss: 0.6163\n",
            "epoch 7.0000 train acc: 0.6863,train loss: 0.6330, dev acc: 0.9050, dev loss: 0.6050\n",
            "epoch 8.0000 train acc: 0.7300,train loss: 0.6173, dev acc: 0.8712, dev loss: 0.5942\n",
            "epoch 9.0000 train acc: 0.7475,train loss: 0.6016, dev acc: 0.9350, dev loss: 0.5816\n",
            "epoch 10.0000 train acc: 0.7800,train loss: 0.5912, dev acc: 0.9587, dev loss: 0.5698\n",
            "epoch 11.0000 train acc: 0.7887,train loss: 0.5799, dev acc: 0.9700, dev loss: 0.5550\n",
            "epoch 12.0000 train acc: 0.7738,train loss: 0.5774, dev acc: 0.9775, dev loss: 0.5451\n",
            "epoch 13.0000 train acc: 0.7937,train loss: 0.5615, dev acc: 0.9775, dev loss: 0.5319\n",
            "epoch 14.0000 train acc: 0.8225,train loss: 0.5464, dev acc: 0.9912, dev loss: 0.5164\n",
            "epoch 15.0000 train acc: 0.8363,train loss: 0.5280, dev acc: 0.9975, dev loss: 0.5019\n",
            "epoch 16.0000 train acc: 0.8413,train loss: 0.5109, dev acc: 0.9975, dev loss: 0.4870\n",
            "epoch 17.0000 train acc: 0.8413,train loss: 0.5033, dev acc: 0.9975, dev loss: 0.4717\n",
            "epoch 18.0000 train acc: 0.8488,train loss: 0.4789, dev acc: 0.9988, dev loss: 0.4535\n",
            "epoch 19.0000 train acc: 0.8375,train loss: 0.4822, dev acc: 1.0000, dev loss: 0.4406\n",
            "epoch 20.0000 train acc: 0.8363,train loss: 0.4633, dev acc: 1.0000, dev loss: 0.4231\n",
            "epoch 21.0000 train acc: 0.8300,train loss: 0.4638, dev acc: 1.0000, dev loss: 0.4071\n",
            "epoch 22.0000 train acc: 0.8562,train loss: 0.4275, dev acc: 1.0000, dev loss: 0.3907\n",
            "epoch 23.0000 train acc: 0.8925,train loss: 0.3994, dev acc: 1.0000, dev loss: 0.3713\n",
            "epoch 24.0000 train acc: 0.8538,train loss: 0.4035, dev acc: 1.0000, dev loss: 0.3546\n",
            "epoch 25.0000 train acc: 0.8625,train loss: 0.3960, dev acc: 1.0000, dev loss: 0.3375\n",
            "epoch 26.0000 train acc: 0.8900,train loss: 0.3510, dev acc: 1.0000, dev loss: 0.3193\n",
            "epoch 27.0000 train acc: 0.8675,train loss: 0.3627, dev acc: 1.0000, dev loss: 0.3028\n",
            "epoch 28.0000 train acc: 0.8962,train loss: 0.3459, dev acc: 1.0000, dev loss: 0.2873\n",
            "epoch 29.0000 train acc: 0.9062,train loss: 0.3167, dev acc: 1.0000, dev loss: 0.2717\n",
            "epoch 30.0000 train acc: 0.8988,train loss: 0.3128, dev acc: 1.0000, dev loss: 0.2550\n",
            "epoch 31.0000 train acc: 0.9137,train loss: 0.2934, dev acc: 1.0000, dev loss: 0.2384\n",
            "epoch 32.0000 train acc: 0.9313,train loss: 0.2834, dev acc: 1.0000, dev loss: 0.2262\n",
            "epoch 33.0000 train acc: 0.9125,train loss: 0.2998, dev acc: 1.0000, dev loss: 0.2128\n",
            "epoch 34.0000 train acc: 0.9300,train loss: 0.2705, dev acc: 1.0000, dev loss: 0.1993\n",
            "epoch 35.0000 train acc: 0.9400,train loss: 0.2498, dev acc: 1.0000, dev loss: 0.1886\n",
            "epoch 36.0000 train acc: 0.9437,train loss: 0.2415, dev acc: 1.0000, dev loss: 0.1779\n",
            "epoch 37.0000 train acc: 0.9263,train loss: 0.2564, dev acc: 1.0000, dev loss: 0.1660\n",
            "epoch 38.0000 train acc: 0.9475,train loss: 0.2327, dev acc: 1.0000, dev loss: 0.1542\n",
            "epoch 39.0000 train acc: 0.9513,train loss: 0.2258, dev acc: 1.0000, dev loss: 0.1470\n",
            "epoch 40.0000 train acc: 0.9550,train loss: 0.2211, dev acc: 1.0000, dev loss: 0.1351\n",
            "epoch 41.0000 train acc: 0.9537,train loss: 0.2013, dev acc: 1.0000, dev loss: 0.1267\n",
            "epoch 42.0000 train acc: 0.9637,train loss: 0.1897, dev acc: 1.0000, dev loss: 0.1173\n",
            "epoch 43.0000 train acc: 0.9500,train loss: 0.1878, dev acc: 1.0000, dev loss: 0.1123\n",
            "epoch 44.0000 train acc: 0.9487,train loss: 0.1925, dev acc: 1.0000, dev loss: 0.1057\n",
            "epoch 45.0000 train acc: 0.9425,train loss: 0.1959, dev acc: 1.0000, dev loss: 0.0980\n",
            "epoch 46.0000 train acc: 0.9613,train loss: 0.1641, dev acc: 1.0000, dev loss: 0.0917\n",
            "epoch 47.0000 train acc: 0.9600,train loss: 0.1657, dev acc: 1.0000, dev loss: 0.0849\n",
            "epoch 48.0000 train acc: 0.9575,train loss: 0.1544, dev acc: 1.0000, dev loss: 0.0825\n",
            "epoch 49.0000 train acc: 0.9563,train loss: 0.1575, dev acc: 1.0000, dev loss: 0.0750\n",
            "epoch 50.0000 train acc: 0.9513,train loss: 0.1552, dev acc: 1.0000, dev loss: 0.0713\n",
            "epoch 51.0000 train acc: 0.9700,train loss: 0.1336, dev acc: 1.0000, dev loss: 0.0671\n",
            "epoch 52.0000 train acc: 0.9537,train loss: 0.1395, dev acc: 1.0000, dev loss: 0.0633\n",
            "epoch 53.0000 train acc: 0.9575,train loss: 0.1329, dev acc: 1.0000, dev loss: 0.0610\n",
            "epoch 54.0000 train acc: 0.9563,train loss: 0.1326, dev acc: 1.0000, dev loss: 0.0565\n",
            "epoch 55.0000 train acc: 0.9550,train loss: 0.1271, dev acc: 1.0000, dev loss: 0.0508\n",
            "epoch 56.0000 train acc: 0.9688,train loss: 0.1175, dev acc: 1.0000, dev loss: 0.0498\n",
            "epoch 57.0000 train acc: 0.9500,train loss: 0.1305, dev acc: 1.0000, dev loss: 0.0472\n",
            "epoch 58.0000 train acc: 0.9625,train loss: 0.1189, dev acc: 1.0000, dev loss: 0.0455\n",
            "epoch 59.0000 train acc: 0.9487,train loss: 0.1233, dev acc: 1.0000, dev loss: 0.0410\n",
            "epoch 60.0000 train acc: 0.9400,train loss: 0.1284, dev acc: 1.0000, dev loss: 0.0400\n",
            "epoch 61.0000 train acc: 0.9537,train loss: 0.1169, dev acc: 1.0000, dev loss: 0.0366\n",
            "epoch 62.0000 train acc: 0.9513,train loss: 0.1127, dev acc: 1.0000, dev loss: 0.0346\n",
            "epoch 63.0000 train acc: 0.9537,train loss: 0.1108, dev acc: 1.0000, dev loss: 0.0343\n",
            "epoch 64.0000 train acc: 0.9587,train loss: 0.1026, dev acc: 1.0000, dev loss: 0.0318\n",
            "epoch 65.0000 train acc: 0.9500,train loss: 0.1132, dev acc: 1.0000, dev loss: 0.0303\n",
            "epoch 66.0000 train acc: 0.9525,train loss: 0.1058, dev acc: 1.0000, dev loss: 0.0285\n",
            "epoch 67.0000 train acc: 0.9513,train loss: 0.1048, dev acc: 1.0000, dev loss: 0.0288\n",
            "epoch 68.0000 train acc: 0.9563,train loss: 0.0979, dev acc: 1.0000, dev loss: 0.0253\n",
            "epoch 69.0000 train acc: 0.9675,train loss: 0.0822, dev acc: 1.0000, dev loss: 0.0246\n",
            "epoch 70.0000 train acc: 0.9575,train loss: 0.0917, dev acc: 1.0000, dev loss: 0.0230\n",
            "epoch 71.0000 train acc: 0.9663,train loss: 0.0798, dev acc: 1.0000, dev loss: 0.0215\n",
            "epoch 72.0000 train acc: 0.9613,train loss: 0.0853, dev acc: 1.0000, dev loss: 0.0202\n",
            "epoch 73.0000 train acc: 0.9625,train loss: 0.0829, dev acc: 1.0000, dev loss: 0.0192\n",
            "epoch 74.0000 train acc: 0.9613,train loss: 0.0838, dev acc: 1.0000, dev loss: 0.0195\n",
            "epoch 75.0000 train acc: 0.9537,train loss: 0.0865, dev acc: 1.0000, dev loss: 0.0186\n",
            "epoch 76.0000 train acc: 0.9487,train loss: 0.0955, dev acc: 1.0000, dev loss: 0.0176\n",
            "epoch 77.0000 train acc: 0.9487,train loss: 0.0929, dev acc: 1.0000, dev loss: 0.0162\n",
            "epoch 78.0000 train acc: 0.9475,train loss: 0.0957, dev acc: 1.0000, dev loss: 0.0160\n",
            "epoch 79.0000 train acc: 0.9563,train loss: 0.0780, dev acc: 1.0000, dev loss: 0.0152\n",
            "epoch 80.0000 train acc: 0.9600,train loss: 0.0753, dev acc: 1.0000, dev loss: 0.0146\n",
            "epoch 81.0000 train acc: 0.9450,train loss: 0.0954, dev acc: 1.0000, dev loss: 0.0144\n",
            "epoch 82.0000 train acc: 0.9563,train loss: 0.0776, dev acc: 1.0000, dev loss: 0.0146\n",
            "epoch 83.0000 train acc: 0.9575,train loss: 0.0752, dev acc: 1.0000, dev loss: 0.0128\n",
            "epoch 84.0000 train acc: 0.9600,train loss: 0.0725, dev acc: 1.0000, dev loss: 0.0126\n",
            "epoch 85.0000 train acc: 0.9600,train loss: 0.0766, dev acc: 1.0000, dev loss: 0.0126\n",
            "epoch 86.0000 train acc: 0.9313,train loss: 0.1005, dev acc: 1.0000, dev loss: 0.0129\n",
            "epoch 87.0000 train acc: 0.9525,train loss: 0.0771, dev acc: 1.0000, dev loss: 0.0112\n",
            "epoch 88.0000 train acc: 0.9563,train loss: 0.0775, dev acc: 1.0000, dev loss: 0.0106\n",
            "epoch 89.0000 train acc: 0.9675,train loss: 0.0616, dev acc: 1.0000, dev loss: 0.0106\n",
            "epoch 90.0000 train acc: 0.9425,train loss: 0.0828, dev acc: 1.0000, dev loss: 0.0107\n",
            "epoch 91.0000 train acc: 0.9587,train loss: 0.0753, dev acc: 1.0000, dev loss: 0.0099\n",
            "epoch 92.0000 train acc: 0.9575,train loss: 0.0712, dev acc: 1.0000, dev loss: 0.0096\n",
            "epoch 93.0000 train acc: 0.9600,train loss: 0.0716, dev acc: 1.0000, dev loss: 0.0091\n",
            "epoch 94.0000 train acc: 0.9425,train loss: 0.0844, dev acc: 1.0000, dev loss: 0.0087\n",
            "epoch 95.0000 train acc: 0.9563,train loss: 0.0704, dev acc: 1.0000, dev loss: 0.0086\n",
            "epoch 96.0000 train acc: 0.9587,train loss: 0.0665, dev acc: 1.0000, dev loss: 0.0090\n",
            "epoch 97.0000 train acc: 0.9463,train loss: 0.0829, dev acc: 1.0000, dev loss: 0.0087\n",
            "epoch 98.0000 train acc: 0.9525,train loss: 0.0755, dev acc: 1.0000, dev loss: 0.0080\n",
            "epoch 99.0000 train acc: 0.9575,train loss: 0.0685, dev acc: 1.0000, dev loss: 0.0076\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    batch_size = 32\n",
        "    lr=10e-4\n",
        "    #lr=1e-4#loss:11.72 10.74\n",
        "    #lr=1e-3#9.6519\n",
        "    #lr=0.01#8.3690\n",
        "    #lr=0.1#8.2 7.72 7.71 ..7156.7147\n",
        "    log_dir='/content/drive/My Drive/model.pth'\n",
        "    #数据集加载\n",
        "    dataset_path = '/content/drive/My Drive/'\n",
        "    x_train, y_train, x_dev, y_dev = get_data(dataset_path, 'TrainData3.mat', 0.2)\n",
        "    #print(x_train[0])\n",
        "    train_dataset = MyDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    dev_dataset = MyDataset(x_dev, y_dev)\n",
        "    dev_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = DnCNN()\n",
        "    #model = Model()\n",
        "    #模型加载\n",
        "    start_epoch=0\n",
        "    '''\n",
        "    if os.path.exists(log_dir):\n",
        "        checkpoint = torch.load(log_dir)\n",
        "        model.load_state_dict(checkpoint['net'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print('加载 epoch {} 成功！'.format(start_epoch))\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        print('无保存模型，将从头开始训练！')\n",
        "    '''\n",
        "    sgd = SGD(model.parameters(), lr)\n",
        "\n",
        "    cost = CrossEntropyLoss()\n",
        "    criterion = MSELoss(reduction='sum')\n",
        "    epoch =100\n",
        "    use_GPU = True\n",
        "    if use_GPU:\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    epoch_train_loss_list = []\n",
        "    epoch_dev_loss_list = []\n",
        "    epoch_train_acc_list = []\n",
        "    epoch_dev_acc_list = []\n",
        "\n",
        "    for _epoch in range(epoch):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        epoch_dev_loss = 0\n",
        "        epoch_train_acc = 0\n",
        "        epoch_dev_acc = 0\n",
        "        train_num=0\n",
        "        dev_num = 0\n",
        "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
        "            s = train_label.shape[0]\n",
        "            sgd.zero_grad()\n",
        "            predict_y = model(train_x.to(device))\n",
        "            #print(train_label.size())\n",
        "            #print(predict_y.size())\n",
        "            #loss = cost(predict_y, train_label.to(device))\n",
        "            loss = F.cross_entropy(predict_y, train_label.to(device))\n",
        "            epoch_train_loss += loss.item()\n",
        "            label_pred = np.argmax(predict_y.cpu().data.numpy(), axis=1)\n",
        "            acc = np.sum(label_pred == train_label.numpy())\n",
        "            # print(\"batch Train acc:\",acc / s)\n",
        "            epoch_train_acc += acc / s\n",
        "            train_num+=1\n",
        "            loss.backward()\n",
        "            sgd.step()\n",
        "\n",
        "        correct = 0\n",
        "        _sum = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (dev_x, dev_label) in enumerate(dev_loader):\n",
        "                s = dev_label.shape[0]\n",
        "                predict_y = model(dev_x.to(device))\n",
        "                # print(predict_y[0], dev_label[0])\n",
        "                loss = cost(predict_y, dev_label.to(device))\n",
        "                epoch_dev_loss += loss.item()\n",
        "                label_pred = np.argmax(predict_y.cpu().data.numpy(), axis=1)\n",
        "                # print(\"------\")\n",
        "                # print(label_pred)\n",
        "                # print(dev_label.numpy())\n",
        "                # print(\"------\")\n",
        "                acc = np.sum(label_pred == dev_label.numpy())\n",
        "                batch_acc=acc / s\n",
        "                dev_num+=1\n",
        "                # print(\"batch_acc::\",batch_acc)\n",
        "                epoch_dev_acc += acc / s\n",
        "                # print(\"devacc\", acc);\n",
        "        epoch_train_loss_list.append(epoch_train_loss / train_num)\n",
        "        epoch_dev_loss_list.append(epoch_dev_loss / train_num)\n",
        "        epoch_train_acc_list.append(epoch_train_acc / dev_num)\n",
        "        epoch_dev_acc_list.append(epoch_dev_acc / dev_num)\n",
        "        print(\"epoch {:.4f} train acc: {:.4f},train loss: {:.4f}, dev acc: {:.4f}, dev loss: {:.4f}\".format(_epoch,epoch_train_acc / train_num, epoch_train_loss / train_num,epoch_dev_acc / dev_num, epoch_dev_loss / dev_num))\n",
        "    t = np.arange(1, len(epoch_train_loss_list) + 1)\n",
        "    acc_plot = plt.subplot(2, 2, 1)\n",
        "    plt.title('acc')\n",
        "    plt.plot(t, epoch_train_acc_list, color='red', label='train acc')\n",
        "    plt.plot(t, epoch_dev_acc_list, color='blue', label='dev acc')\n",
        "    loss_plot = plt.subplot(2, 2, 2)\n",
        "    plt.title('loss ')\n",
        "    plt.plot(t, epoch_train_loss_list, color='red', label='train loss')\n",
        "    plt.plot(t, epoch_dev_loss_list, color='skyblue', label='dev loss')\n",
        "    plt.savefig('/content/drive/My Drive/040907')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    acc_plot = plt.subplot(2, 2, 1)\n",
        "    plt.title('acc')\n",
        "    plt.plot(t, epoch_train_acc_list, color='skyblue', label='train acc')\n",
        "    plt.xlabel(\"epcho\")\n",
        "\n",
        "    loss_plot = plt.subplot(2, 2, 2)\n",
        "    plt.title('loss ')\n",
        "    plt.plot(t, epoch_train_loss_list, color='skyblue', label='train loss')\n",
        "    plt.xlabel(\"epcho\")\n",
        "    plt.savefig('/content/drive/My Drive/040907.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "px1s0sYqk-Eu",
        "outputId": "5b35ad16-3764-43eb-bdd7-6578ced0685a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-863567bab7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skyblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epcho\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}